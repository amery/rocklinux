# --- ROCK-COPYRIGHT-NOTE-BEGIN ---
# 
# This copyright note is auto-generated by ./scripts/Create-CopyPatch.
# Please add additional copyright information _after_ the line containing
# the ROCK-COPYRIGHT-NOTE-END tag. Otherwise it might get removed by
# the ./scripts/Create-CopyPatch script. Do not edit this copyright text!
# 
# ROCK Linux: rock-src/package/gnome2/gimp/gcc40.patch
# ROCK Linux is Copyright (C) 1998 - 2006 Clifford Wolf
# 
# This patch file is dual-licensed. It is available under the license the
# patched project is licensed under, as long as it is an OpenSource license
# as defined at http://www.opensource.org/ (e.g. BSD, X11) or under the terms
# of the GNU General Public License as published by the Free Software
# Foundation; either version 2 of the License, or (at your option) any later
# version.
# 
# --- ROCK-COPYRIGHT-NOTE-END ---

Fixes for errors like:
gimp-composite-mmx.c: In function 'gimp_composite_addition_rgba8_rgba8_rgba8_mmx':
gimp-composite-mmx.c:94: error: unknown register name '%mm0' in 'asm'
gimp-composite-mmx.c:101: error: unknown register name '%mm4' in 'asm'
gimp-composite-mmx.c:101: error: unknown register name '%mm3' in 'asm'
gimp-composite-mmx.c:101: error: unknown register name '%mm2' in 'asm'
gimp-composite-mmx.c:101: error: unknown register name '%mm1' in 'asm'

diff -dur gimp-2.2.8/app/composite/gimp-composite-mmx.c gimp-2.2.8-p/app/composite/gimp-composite-mmx.c
--- gimp-2.2.8/app/composite/gimp-composite-mmx.c	2005-01-11 12:27:25.000000000 +0100
+++ gimp-2.2.8-p/app/composite/gimp-composite-mmx.c	2005-10-22 13:37:05.000000000 +0200
@@ -94,7 +94,7 @@
   asm volatile ("movq    %0,%%mm0"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64)
-                : "%mm0");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -110,7 +110,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -130,7 +130,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -192,7 +192,7 @@
                     "\tmovq      %%mm7,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
       d++;
       b++;
       a++;
@@ -246,7 +246,7 @@
                     "\tmovd      %%mm7,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    : pdivwqX_clobber);
     }
 
   asm("emms");
@@ -269,7 +269,7 @@
                     "\tmovq    %%mm2, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -283,7 +283,7 @@
                     "\tmovd    %%mm2, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -297,7 +297,7 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -316,7 +316,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -339,7 +339,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -358,7 +358,7 @@
                 "\tmovq    %1, %%mm7\n"
                 :
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w1_64)
-                : "%mm0", "%mm7");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -398,7 +398,7 @@
                     "\tmovq      %%mm3,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_alpha_mask_64)
-                    : pdivwuqX_clobber, "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    : pdivwuqX_clobber);
       a++;
       b++;
       d++;
@@ -442,7 +442,7 @@
                     "\tmovd      %%mm3,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_alpha_mask_64)
-                    : pdivwuqX_clobber, "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    : pdivwuqX_clobber);
     }
 
   asm("emms");
@@ -495,7 +495,7 @@
                     "\tmovq      %%mm7,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_w256_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwuqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    : pdivwuqX_clobber);
       a++;
       b++;
       d++;
@@ -538,7 +538,7 @@
                     "\tmovd      %%mm7,%2\n"
                     : /* empty */
                     : "m" (*a), "m" (*b), "m" (*d), "m" (*rgba8_w256_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwuqX_clobber, "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    : pdivwuqX_clobber);
     }
 
   asm("emms");
@@ -558,7 +558,7 @@
                 "movq       %1,%%mm7\n"
                 : /* no outputs */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64)
-                : "%mm0",  "%mm7", "%mm6");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -589,7 +589,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -620,7 +620,7 @@
                     "\tmovd      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -639,7 +639,7 @@
                 "movq    %1, %%mm7\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64)
-                : "%mm0", "%mm6", "%mm7");
+);
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -667,7 +668,7 @@
                     "\tmovq      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+);
       a++;
       b++;
       d++;
@@ -697,7 +699,7 @@
                     "\tmovd      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -711,7 +713,7 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -727,7 +729,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -750,7 +752,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -770,7 +772,7 @@
                 "pxor    %%mm6,%%mm6\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64)
-                : "%mm6", "%mm7", "%mm0");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -797,7 +799,7 @@
                     "\tmovq      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -824,7 +826,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
   }
 
   asm("emms");
@@ -892,7 +894,7 @@
   asm volatile ("pxor    %%mm0,%%mm0\n"
                 "movq       %0,%%mm7"
                 : /* empty */
-                : "m" (*rgba8_w128_64) : "%mm0");
+                : "m" (*rgba8_w128_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -943,7 +945,7 @@
                     "\tmovq      %%mm1,%2\n"
                     : "+m" (*a), "+m" (*b), "+m" (*d)
                     : "m" (*rgba8_w2_64), "m" (*rgba8_alpha_mask_64)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -998,7 +1000,7 @@
                     "\tmovd      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_w2_64), "m" (*rgba8_alpha_mask_64)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -1024,7 +1026,7 @@
                 "\tmovq     %1,%%mm7\n"
                 : /* empty */
                 : "m" (_op->scale.scale), "m" (*rgba8_w128_64)
-                : "%eax", "%ebx", "%mm0", "%mm5", "%mm6", "%mm7");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -1046,7 +1048,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
       a++;
       d++;
     }
@@ -1064,7 +1066,7 @@
                     "\tmovd      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
   }
 
   asm("emms");
@@ -1083,7 +1085,7 @@
                 "movq       %1,%%mm7\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64)
-                : "%mm0", "%mm6", "%mm7");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -1134,7 +1136,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -1189,7 +1191,7 @@
                     "\tmovd      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm volatile ("emms");
@@ -1204,7 +1206,7 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -1224,7 +1226,7 @@
                     "\tmovq    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -1248,7 +1250,7 @@
                     "\tmovd    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm volatile ("emms");
@@ -1269,7 +1271,7 @@
                     "\tmovq    %%mm2,%1\n"
                     : "+m" (*a), "+m" (*b)
                     : 
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
     }
@@ -1282,7 +1284,7 @@
                     "\tmovd    %%mm2,%1\n"
                     : "+m" (*a), "+m" (*b)
                     :
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
diff -dur gimp-2.2.8/app/composite/gimp-composite-sse2.c gimp-2.2.8-p/app/composite/gimp-composite-sse2.c
--- gimp-2.2.8/app/composite/gimp-composite-sse2.c	2005-05-06 14:27:50.000000000 +0200
+++ gimp-2.2.8-p/app/composite/gimp-composite-sse2.c	2005-10-22 13:49:02.000000000 +0200
@@ -100,7 +100,7 @@
                 "\tmovq      %1,%%mm0"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_128), "m" (*rgba8_alpha_mask_64)
-                : "%xmm0", "%mm0");
+                );
 
   for (; n_pixels >= 4; n_pixels -= 4)
     {
@@ -117,7 +117,7 @@
                     "\tmovdqu  %%xmm1,%0\n"
                     : "=m" (*D)
                     : "m" (*A), "m" (*B)
-                    : "%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6", "%xmm7");
+                    );
       A++;
       B++;
       D++;
@@ -141,7 +141,7 @@
                     "\tmovq    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
       a++;
       b++;
       d++;
@@ -161,7 +161,7 @@
                     "\tmovd    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
     }
 
   asm("emms");
@@ -195,7 +195,7 @@
                     "\tmovdqu      %%xmm2,%0\n"
                     : "=m" (*D)
                     : "m" (*A), "m" (*B)
-                    : "%xmm1", "%xmm2", "%xmm3", "%xmm4");
+                    );
       A++;
       B++;
       D++;
@@ -212,7 +212,7 @@
                     "\tmovq    %%mm2, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -226,7 +226,7 @@
                     "\tmovd    %%mm2, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -247,7 +247,7 @@
                 "\tmovdqu %1,%%xmm0"
                 :               /*  */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_alpha_mask_128)
-                : "%mm0", "%xmm0");
+                );
 
   for (; n_pixels >= 4; n_pixels -= 4)
     {
@@ -266,7 +266,7 @@
                     "\tmovdqu    %%xmm1,%0\n"
                     : "=m" (*D)
                     : "m" (*A), "m" (*B)
-                    : "%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5");
+                    );
       A++;
       B++;
       D++;
@@ -293,7 +293,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -316,7 +316,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -366,7 +366,7 @@
                     "\tmovdqu    %%xmm7,%0\n"
                     : "=m" (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_w256_128), "m" (*rgba8_alpha_mask_128)
-                    : "%eax", "%ecx", "%edx", "%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6", "%xmm7");
+                    );
       op.A += 16;
       op.B += 16;
       op.D += 16;
@@ -409,7 +409,7 @@
                     "\tmovq      %%mm7,%0\n"
                     : (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_w256_64), "m" (*rgba8_alpha_mask_64)
-                    : "%eax", "%ecx", "%edx", "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
       op.A += 8;
       op.B += 8;
       op.D += 8;
@@ -452,7 +452,7 @@
                     "\tmovd      %%mm7,%0\n"
                     : "=m" (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_w256_64), "m" (*rgba8_alpha_mask_64)
-                    : "%eax", "%ecx", "%edx", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -478,7 +478,7 @@
                 "\tmovdqu     %3,%%xmm7\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64), "m" (*rgba8_alpha_mask_128), "m" (*rgba8_w128_128)
-                : "%mm0", "%mm6", "%mm7", "%xmm0", "%xmm6", "%xmm7");
+                );
 
   for (; n_pixels >= 4; n_pixels -= 4)
     {
@@ -509,7 +509,7 @@
                     "\tmovdqu    %%xmm1,%0\n"
                     : "=m" (*D)
                     : "m" (*A), "m" (*B)
-                    : "%xmm1", "%xmm2", "%xmm3", "%xmm4");
+                    );
       A++;
       B++;
       D++;
@@ -548,7 +548,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -573,7 +573,7 @@
                     "\tmovd      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -590,7 +590,7 @@
   uint128 *B = (uint128 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movdqu    %0,%%xmm0"  :  : "m" (*rgba8_alpha_mask_64) : "%xmm0");
+  asm volatile ("movdqu    %0,%%xmm0"  :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 4; n_pixels -= 4)
     {
@@ -606,7 +606,7 @@
                     "\tmovdqu  %%xmm1, %0\n"
                     : "=m" (*D)
                     : "m" (*A), "m" (*B)
-                    : "%xmm1", "%xmm2", "%xmm3", "%xmm4");
+                    );
       A++;
       B++;
       D++;
@@ -630,7 +630,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -650,7 +650,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -671,7 +671,7 @@
                 "\tmovdqu  %1,%%xmm0\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_alpha_mask_128)
-                : "%mm0", "%xmm0");
+                );
 
   for (; n_pixels >= 4; n_pixels -= 4)
     {
@@ -688,7 +688,7 @@
                     "\tmovdqu   %%xmm1,%0\n"
                     : "=m" (*D)
                     : "m" (*A), "m" (*B)
-                    : "%xmm1", "%xmm2", "%xmm3", "%xmm4");
+                    );
       A++;
       B++;
       D++;
@@ -712,7 +712,7 @@
                     "\tmovq    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -732,7 +732,7 @@
                     "\tmovd    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -772,7 +772,6 @@
                       "+m" (op.A[2]), "+m" (op.B[2]),
                       "+m" (op.A[3]), "+m" (op.B[3])
                     : /* empty */
-                    : "%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6", "%xmm7"
                     );
 #else
       asm volatile ("  movdqu      %0,%%xmm0\n"
@@ -828,7 +827,7 @@
                     "\tmovdqu  %%xmm2,%1\n"
                     : "+m" (*op.A), "+m" (*op.B)
                     : /* empty */
-                    : "%xmm2", "%xmm3");
+                    );
       op.A += 16;
       op.B += 16;
     }
@@ -841,7 +840,7 @@
                     "\tmovq   %%mm2,%1\n"
                     : "+m" (*op.A), "+m" (*op.B)
                     : /* empty */
-                    : "%mm2", "%mm3");
+                    );
       op.A += 8;
       op.B += 8;
     }
@@ -854,7 +853,7 @@
                     "\tmovd   %%mm2,%1\n"
                     : "+m" (*op.A), "+m" (*op.B)
                     : /* empty */
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
diff -dur gimp-2.2.8/app/composite/gimp-composite-sse.c gimp-2.2.8-p/app/composite/gimp-composite-sse.c
--- gimp-2.2.8/app/composite/gimp-composite-sse.c	2005-01-08 00:58:33.000000000 +0100
+++ gimp-2.2.8-p/app/composite/gimp-composite-sse.c	2005-10-22 13:42:12.000000000 +0200
@@ -72,7 +72,7 @@
   asm volatile ("movq    %0,%%mm0"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64)
-                : "%mm0");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -88,7 +88,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -108,7 +108,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -171,7 +171,7 @@
                     "\tmovq      %%mm7,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    : pdivwqX_clobber);
       d++;
       b++;
       a++;
@@ -225,7 +225,7 @@
                     "\tmovd      %%mm7,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b), "m" (*rgba8_b255_64), "m" (*rgba8_w1_64), "m" (*rgba8_w255_64), "m" (*rgba8_alpha_mask_64)
-                    : pdivwqX_clobber, "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    : pdivwqX_clobber);
     }
 
   asm("emms");
@@ -248,7 +248,7 @@
                     "\tmovq    %%mm2, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -262,7 +262,7 @@
                     "\tmovd    %%mm2, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -276,7 +276,7 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -295,7 +295,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -318,7 +318,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -334,7 +334,7 @@
                 "\tmovq    %1, %%mm7\n"
                 :
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w1_64)
-                : "%mm0", "%mm7");
+                );
 
   for (; op.n_pixels >= 2; op.n_pixels -= 2)
     {
@@ -374,7 +374,7 @@
                     "\tmovq      %%mm3,%0\n"
                     : "=m" (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_alpha_mask_64)
-                    : "%eax", "%ecx", "%edx", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       op.A += 8;
       op.B += 8;
       op.D += 8;
@@ -419,7 +419,7 @@
                     "\tmovd      %%mm3,%0\n"
                     : "=m" (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_alpha_mask_64)
-                    : "%eax", "%ecx", "%edx", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -469,7 +469,7 @@
                     "\tmovq      %%mm7,%0\n"
                     : "=m" (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_w256_64), "m" (*rgba8_alpha_mask_64)
-                    : "%eax", "%ecx", "%edx", "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
       op.A += 8;
       op.B += 8;
       op.D += 8;
@@ -512,7 +512,7 @@
                     "\tmovd      %%mm7,%2\n"
                     : "=m" (*op.D)
                     : "m" (*op.A), "m" (*op.B), "m" (*rgba8_w256_64), "m" (*rgba8_alpha_mask_64)
-                    : "%eax", "%ecx", "%edx", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -533,7 +533,7 @@
                 "\tmovq       %1,%%mm7\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64)
-                : "%mm0", "%mm6", "%mm7");
+                );
 
 
   for (; n_pixels >= 2; n_pixels -= 2)
@@ -565,7 +565,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -597,7 +597,7 @@
                     "\tmovd      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -616,7 +616,7 @@
                 "movq    %1, %%mm7\n"
                 : /* empty */
                 : "m" (*rgba8_alpha_mask_64), "m" (*rgba8_w128_64)
-                : "%mm0", "%mm6", "%mm7");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -644,7 +644,7 @@
                     "\tmovq      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -675,7 +675,7 @@
                     "\tmovd      %%mm1, %0\n"
                     : "+m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -689,7 +689,7 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -705,7 +705,7 @@
                     "\tmovq    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -728,7 +728,7 @@
                     "\tmovd    %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -742,9 +742,9 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
-  asm volatile ("movq    %0,%%mm7"     :  : "m" (*rgba8_w128_64) : "%mm7");
-  asm volatile ("pxor    %%mm6,%%mm6"  :  :  : "%mm6");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
+  asm volatile ("movq    %0,%%mm7"     :  : "m" (*rgba8_w128_64) );
+  asm volatile ("pxor    %%mm6,%%mm6"  :  :  );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -771,7 +771,7 @@
                     "\tmovq      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -798,7 +798,7 @@
                     "\tmovd      %%mm1, %0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -993,7 +993,7 @@
                 "\tmovq     %1,%%mm7\n"
                 : /* empty */
                 : "m" (_op->scale.scale), "m" (*rgba8_w128_64)
-                : "%eax", "%ebx", "%mm0", "%mm3", "%mm5", "%mm6", "%mm7");
+                );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -1015,7 +1015,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a)
-                    : "%mm1", "%mm2", "%mm4", "%mm5", "%mm7");
+                    );
       a++;
       d++;
     }
@@ -1033,7 +1033,7 @@
                     "\tmovd      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a)
-                    : "%mm1", "%mm2", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
     }
 
   asm("emms");
@@ -1047,8 +1047,8 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
-  asm volatile ("movq    %0,%%mm7"     :  : "m" (*rgba8_w128_64)  : "%mm7");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
+  asm volatile ("movq    %0,%%mm7"     :  : "m" (*rgba8_w128_64)  );
   asm volatile ("pxor    %mm6, %mm6");
 
   for (; n_pixels >= 2; n_pixels -= 2)
@@ -1100,7 +1100,7 @@
                     "\tmovq      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
       a++;
       b++;
       d++;
@@ -1155,7 +1155,7 @@
                     "\tmovd      %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4", "%mm5");
+                    );
     }
 
   asm("emms");
@@ -1170,7 +1170,7 @@
   uint64 *b = (uint64 *) _op->B;
   gulong n_pixels = _op->n_pixels;
 
-  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) : "%mm0");
+  asm volatile ("movq    %0,%%mm0"     :  : "m" (*rgba8_alpha_mask_64) );
 
   for (; n_pixels >= 2; n_pixels -= 2)
     {
@@ -1186,7 +1186,7 @@
                     "\tmovq    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
       d++;
@@ -1206,7 +1206,7 @@
                     "\tmovd    %%mm1,%0\n"
                     : "=m" (*d)
                     : "m" (*a), "m" (*b)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -1227,7 +1227,7 @@
                     "\tmovq    %%mm2,%1\n"
                     : "+m" (*a), "+m" (*b)
                     :
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
       a++;
       b++;
     }
@@ -1240,7 +1240,7 @@
                     "\tmovd    %%mm2,%1\n"
                     : "+m" (*a), "+m" (*b)
                     : /* empty */
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   asm("emms");
@@ -1390,7 +1390,7 @@
                     "\tmovq      %%mm7,(%2); addl $8,%2\n"
                     : "+r" (op.A), "+r" (op.B), "+r" (op.D)
                     : "m" (*va8_b255), "m" (*va8_w1), "m" (*va8_w255), "m" (*va8_alpha_mask)
-                    : "%mm1", "%mm2", "%mm3", "%mm4");
+                    );
     }
 
   if (op.n_pixels)
@@ -1441,7 +1441,7 @@
                     "\tmovd      %%mm7,(%2)\n"
                     : /* empty */
                     : "r" (op.A), "r" (op.B), "r" (op.D), "m" (*va8_b255), "m" (*va8_w1), "m" (*va8_w255), "m" (*va8_alpha_mask)
-                    : "%mm0", "%mm1", "%mm2", "%mm3", "%mm4", "%mm5", "%mm6", "%mm7");
+                    );
     }
 
   asm("emms");
